{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6 - CS348 Spring 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description** - In this assignment, you will run and analyze binary classification using ensembles of classification trees.\n",
    "\n",
    "**Getting Started** - You should complete the assignment using your own installation of Python 3 and the packages numpy, pandas, matplotlib, and seaborn. Download the assignment from Moodle and unzip the file. This will create a directory with this file, 'HW06.ipynb', and a folder 'data' containing four csv files.\n",
    "\n",
    "**Deliverables** - The assignment has a single deliverable: this jupyter notebook file saved as a pdf. Please answer all coding and writing questions in the body of this file. Once all of the answers are complete, download the file by navigating the following menus: File -> Download as -> PDF via LaTeX. Submit the downloaded pdf file on gradescope. Alternatively, you can save the file as a pdf via the following: File -> Print Preview -> Print as pdf.\n",
    "\n",
    "**Data Sets** - In this assignment, you will use two datasets, one on handwritten digit classification loaded from the 'data' directory, and the other on breast cancer from the sci-kit learn library.\n",
    "\n",
    "**Academic Honesty Statement** - Copying solutions from external sources (books, web pages, etc.) or other students is considered cheating. Sharing your solutions with other students is considered cheating. Posting your code to public repositories such as GitHub is also considered cheating. Any detected cheating will result in a grade of 0 on the assignment for all students involved, and potentially a grade of F in the course. \n",
    "\n",
    "This academic honesty statement does not restrict you from reading official documentation or using other web resources for understanding the syntax of python, related data science libraries, or properties of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not import any other libraries other than those listed here. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you'll use a Random Forest ensemble model to classify a subset of handwritten digit data with added white-noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.loadtxt('data/x_train.csv', delimiter=',')\n",
    "y_train = np.loadtxt('data/y_train.csv', delimiter=',')\n",
    "x_val = np.loadtxt('data/x_val.csv', delimiter=',')\n",
    "y_val = np.loadtxt('data/y_val.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (15 points)  \n",
    "For all integer values of `n_estimators` between 1 and 50, using sci-kit learn's `RandomForestClassifier` class with `random_state=0`, train a random forest model between features `x_train` and targets `y_train`. For each model compute the classification accuracy on the training data and on the validation data, `x_val` and `y_val`. Using color to distinguish between training and validation accuracy, plot the results in a 2-d plot with `n_estimators` on the horizontal axis and `accuracy` on the vertical axis.\n",
    "\n",
    "Which value of `n_estimators` would you select? What are the advantages and disadvantages of using the random forests classifier over the single classification tree? Be specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 Solution\n",
    "\n",
    "# --- write code here ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (10 points)  \n",
    "As discussed in class, ensemble methods make predictions by computing a weighted average of the individual predictions from a set of weak classifiers. However, if all of the weak classifiers are identical, the ensemble is equivalent to using a single classifier. How does the random forest algorithm introduce variability into the decision rules of each tree?\n",
    "\n",
    "Note: Your answer should include two sources of variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (20 points)  \n",
    "For each value of `max_depth` between 1 and 12, train a random forest and decision tree model using sci-kit learn's `RandomForestClassifier` and `DecisionTreeClassifier` classes using the training data `x_train` and `y_train` with `random_state=0` for both models and `n_estimators=100` for the `RandomForestClassifier`. For each of these models, evaluate the classification accuracy on the validation data `x_val` and `y_val`. Using color to distinguish between the random forest and the decision tree models, plot the results in a 2-d plot with `max_depth` on the horizontal axis and validation accuracy on the vertical axis.\n",
    "\n",
    "How does the relationship between validation accuracy and `max_depth` differ between the two models? What causes this difference?\n",
    "\n",
    "Hint: Consider how bagging and the `max_depth` parameter influence the classifier's accuracy in terms of its bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Part 3 Solution\n",
    "\n",
    "# --- write code here ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Boosted Classification Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "x = data['data']\n",
    "y = data['target']\n",
    "y[y==0] = -1\n",
    "\n",
    "indeces = np.array([468, 342,  40, 303, 459, 225, 523, 250, 438, 365, 538, 330,  35,\n",
    "       120,  87, 338, 250,  26,  36, 407, 531, 330, 226,  18, 473, 121,\n",
    "       210, 234, 431, 536, 202, 319, 509, 235, 400, 107, 266, 562,  50,\n",
    "       209, 507,  24, 280, 178,  90, 171,  44, 208, 536,  74,  41, 162,\n",
    "       469, 352, 535, 189, 262, 133, 436, 562, 278, 434, 510, 346, 559,\n",
    "        16, 112,  82, 554, 549, 537, 347, 192, 186,  84, 439, 172, 496,\n",
    "       522,  25, 388, 474, 410, 529, 478, 225, 298, 323, 170, 308, 161,\n",
    "       411, 184, 341, 233, 244, 142,  57, 310, 187, 177, 494, 241, 522,\n",
    "       250, 565, 328, 468, 516, 373, 434, 180, 537, 549, 546,  51, 509,\n",
    "       390,   7, 416,  28, 544, 252, 115,  55, 388,   4, 275, 144, 237,\n",
    "       123, 419, 232, 277, 102,  44, 225, 493, 364, 116, 232, 559, 426,\n",
    "       254, 534, 548, 405, 374, 435, 122, 288, 212, 441, 545, 279, 130,\n",
    "       366, 447, 260, 568, 328,  96, 524, 368, 269, 418, 280, 137,   0,\n",
    "        59, 208,  22, 468, 433, 479, 261, 355, 543, 390, 537, 404, 455,\n",
    "       147, 524, 554, 455, 202, 356, 278, 440, 280,  52, 552, 501,   0,\n",
    "       177, 505, 107,  58,  51, 504,  47, 102, 267,  10, 247,  86, 215,\n",
    "       423,  64, 251, 183, 439, 438, 315, 527, 287, 487, 538, 337, 301,\n",
    "        21,  23,  89, 272, 400, 402, 435, 438, 263, 357, 426, 188, 427,\n",
    "       324, 318, 388, 357, 108, 536, 393, 467,   6, 283, 205,  32,  58,\n",
    "       516, 170,  46,  69, 522, 351,   9, 480, 341, 428, 439, 299, 206,\n",
    "       347, 421, 113, 400, 129,  40, 371, 243, 327, 382,  73, 450, 472,\n",
    "        95,  92, 284, 502, 556, 414, 159, 360,  96, 167,  45, 455, 435,\n",
    "       118,  78, 359, 256,  80, 250, 206,  25, 259, 528, 145, 174, 388,\n",
    "       355, 119, 512, 376, 365, 230, 389,  84, 376, 174, 178, 486, 278,\n",
    "       138, 432, 511, 374,  24, 181, 140, 207,  50,  52, 566, 186, 435,\n",
    "       551, 240, 406, 233, 234, 148, 123, 144, 522,  32, 547, 223, 426,\n",
    "        43, 453, 343, 443,  45, 413, 315, 265,  81, 267, 148, 114, 379,\n",
    "       365, 456, 204,  29, 198, 270,  20,  71, 441, 475, 187, 187, 121,\n",
    "       210, 255, 513,  49, 203, 131, 211, 436, 479, 568, 297, 172, 444,\n",
    "       298,   7, 345, 539, 407, 370, 235,   9, 307, 140, 433, 423,  63,\n",
    "       556, 129, 363, 137,  97,  26, 126, 311, 294, 349, 253, 326, 323,\n",
    "       334, 245, 128, 175, 568, 182, 245, 273, 519, 491, 481, 170, 438,\n",
    "       353, 531, 252,  73, 437, 408, 209, 246, 248,  12, 525, 423, 476,\n",
    "       543,  16, 414, 491, 404, 417, 122, 135, 354, 207, 323,  93, 261,\n",
    "       260,  77, 154, 276, 257, 546,  67, 309,  96, 227, 393, 461, 323,\n",
    "       418, 333, 109, 108,  57, 222, 275, 413, 110, 549, 290, 524, 470,\n",
    "       298,   0, 317, 455, 492, 191, 132, 277, 525, 282, 525, 486, 320,\n",
    "       324, 377, 178, 494, 382, 477, 180, 392, 254, 454, 113, 168, 176,\n",
    "       219, 234, 194, 356, 145, 494, 189, 253, 208, 417, 117, 253, 521,\n",
    "        46, 389, 466, 123, 153, 495, 241, 398, 522, 245, 499, 563, 308,\n",
    "        21,   5, 119,   7, 488, 178, 557, 368, 244, 379, 223,  45, 500,\n",
    "        90, 291, 428, 418, 183, 394, 410, 521, 458, 496, 441, 190,  61,\n",
    "       558, 133,  80,  43, 272, 190, 334, 158,  46, 532,   4,  46, 136,\n",
    "       530,  93,  64, 559, 506,  65, 177, 241, 118, 249])\n",
    "\n",
    "split_point = int(x.shape[0]/2)\n",
    "\n",
    "x_train = x[indeces][:split_point]\n",
    "y_train = y[indeces][:split_point]\n",
    "x_val = x[indeces][split_point:]\n",
    "y_val = y[indeces][split_point:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (45 points)  \n",
    "Using sci-kit learn's `DecisionTreeClassifier` class as a \"weak learner\", implement the AdaBoost algorithm as described in the \"Example Algorithm (Discrete AdaBoost)\" section of https://en.wikipedia.org/wiki/AdaBoost. Your implementation should modify the `fit` and `predict` methods of the `AdaBoostTrees` class below. Your implementation can use any built-in methods from the `DecisionTreeClassifier` class in sci-kit learn, but cannot use any of the classes or methods in `sklearn.ensembles`.\n",
    "\n",
    "Once you have implemented the `fit` and `predict` methods for the `AdaBoostTrees` class, compute the classification accuracy on the validation data using the default values of `n_estimators=10` and `max_depth=1`.\n",
    "\n",
    "Note 1: You should not modify the `__init__` method for the `AdaBoostTrees` implementation, but you will need to use the 4 initialized attributes in the `fit` and `predict` methods.\n",
    "\n",
    "Note 2: The class labels in `y_train` have been coded as `[-1, 1]` to be consistent with the description in the wikipedia article. This differs from our standard convention of coding class labels as `[0, 1]`.\n",
    "\n",
    "Hint: You may want to inspect the documentation of the initialization and method arguments for the `DecisionTreeClassifier` object. This documentation can be found at https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 Solution\n",
    "\n",
    "class AdaBoostTrees(object):\n",
    "    \n",
    "#   Note: You will need to remove the \"pass\" statements\n",
    "\n",
    "    def __init__(self, n_estimators=10, max_depth=1):\n",
    "\n",
    "#       self.n_estimators : The number of decision tree classifiers used in the ensemble.\n",
    "#       self.max_depth    : The max_depth setting for each of the DecisionTree objects.\n",
    "#       self.trees        : A list containing all of the DecisionTree objects.\n",
    "#       self.tree_weights : A list containing the weight of each of the trained DecisionTree objects.\n",
    "\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.tree_weights = []\n",
    "    \n",
    "    def fit(self, x_train, y_train):\n",
    "        \n",
    "#       This method takes as input two numpy arrays, x_train and y_train,\n",
    "#       and modifies the AdaBoostTrees object in-place. This method implements\n",
    "#       the AdaBoost algorithm using sci-kit learn's DecisionTreeClassifier\n",
    "#       as the \"weak learner\".\n",
    "\n",
    "#       --- Insert code here ---\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "#       This method takes as input a numpy array of size (N, D) and \n",
    "#       returns a numpy array of predictions of size (N). Calling this \n",
    "#       method requires that fit() has already been run on the training data.    \n",
    "\n",
    "#       --- Insert code here ---\n",
    "\n",
    "        pass\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 Solution\n",
    "\n",
    "# --- Insert code here ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (10 points)  \n",
    "Both the `RandomForestClassifier` and our `AdaBoostTrees` models implement an ensemble of decision tree classifiers. How do these methods differ, and what are the advantages and disadvantages of each. Describe a situation where you would prefer a Random Forest classification method over the Adaptive Boosting approach you implemented in part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
