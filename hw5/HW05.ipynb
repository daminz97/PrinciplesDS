{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - CS348 Spring 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description** - In this assignment, you will run and analyze binary classification using decision trees.\n",
    "\n",
    "**Getting Started** - You should complete the assignment using your own installation of Python 3 and the packages numpy, pandas, matplotlib, and seaborn. Download the assignment from Moodle and unzip the file. This will create a directory with this file, 'HW05.ipynb'.\n",
    "\n",
    "You will also need to install the pydotplus library by running `pip install pydotplus` or `conda install pydotplus` in the terminal.\n",
    "\n",
    "**Deliverables** - The assignment has a single deliverable: this jupyter notebook file saved as a pdf. Please answer all coding and writing questions in the body of this file. Once all of the answers are complete, download the file by navigating the following menus: File -> Download as -> PDF via LaTeX. Submit the downloaded pdf file on gradescope. Alternatively, you can save the file as a pdf via the following: File -> Print Preview -> Print as pdf.\n",
    "\n",
    "**Data Sets** - In this assignment, you will a single dataset from the sci-kit learn repository on breast cancer.\n",
    "\n",
    "**Academic Honesty Statement** - Copying solutions from external sources (books, web pages, etc.) or other students is considered cheating. Sharing your solutions with other students is considered cheating. Posting your code to public repositories such as GitHub is also considered cheating. Any detected cheating will result in a grade of 0 on the assignment for all students involved, and potentially a grade of F in the course. \n",
    "\n",
    "This academic honesty statement does not restrict you from reading official documentation or using other web resources for understanding the syntax of python, related data science libraries, or properties of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not import any other libraries other than those listed here. \n",
    "import pydotplus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.tree._tree import TREE_LEAF\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Decision Tree Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you'll use a Decision Tree model to classify whether a tumor is benign or malignant in a breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data.\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['data']\n",
    "y = data['target']\n",
    "\n",
    "x_train = x[:500]\n",
    "y_train = y[:500]\n",
    "x_val = x[500:]\n",
    "y_val = y[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_leaves(model):\n",
    "    return sum(model.tree_.children_left < 0)\n",
    "\n",
    "def print_decision_tree(model):\n",
    "    # Taken from https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176\n",
    "    dot_data = StringIO()\n",
    "\n",
    "    export_graphviz(model, out_file=dot_data,  \n",
    "                    filled=True, rounded=True,\n",
    "                    special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "    return Image(graph.create_png())\n",
    "\n",
    "# Modifed from David Dale's solution at https://stackoverflow.com/questions/49428469/pruning-decision-trees.\n",
    "def prune_tree(model, threshold):\n",
    "    prune_index(model.tree_, 0, threshold)\n",
    "    return model\n",
    "\n",
    "def prune_index(inner_tree, index, threshold):\n",
    "    # Recursively call prune_index until you hit the leaf nodes.\n",
    "    if inner_tree.children_left[index] != TREE_LEAF:\n",
    "        prune_index(inner_tree, inner_tree.children_left[index], threshold)\n",
    "        prune_index(inner_tree, inner_tree.children_right[index], threshold)\n",
    "    if inner_tree.value[index].sum() < threshold:\n",
    "        # turn node into a leaf by \"unlinking\" its children\n",
    "        inner_tree.children_left[index] = TREE_LEAF\n",
    "        inner_tree.children_right[index] = TREE_LEAF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (10 points)  \n",
    "Using sci-kit learn's `DecisionTreeClassifier` class with `random_state=0`, fit a model between features `x_train` and targets `y_train`. Use the function `print_decision_tree(model)` to visually inspect the trained decision tree model. \n",
    "\n",
    "Using only the printed decision tree, evaluate the following sample probabilities.   \n",
    "\n",
    "`P(y_train=1)`  \n",
    "`P(y_train=1|X22>106.1)`  \n",
    "`P(y_train=0|X22>106.1, X7>0.049)`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 Solution\n",
    "\n",
    "# --- write code here ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (20 points)  \n",
    "Again using sci-kit learn's `DecisionTreeClassifier` class with `random_state=0`, fit a model between features `x_train` and targets `y_train`. Use the function `prune_tree` with a threshold of `200` to prune the trained decision tree model. Use the function `print_decision_tree(model)` to visually inspect the pruned decision tree model.\n",
    "\n",
    "Using only the two printed decision trees, describe an instance of `x` such that the model you trained in part 1 would predict `y=0` and the pruned model would predict class `y=1`. Be specific about variable values.\n",
    "\n",
    "Note: The function `prune_tree(model, threshold)` takes as input a fully trained decision tree model and returns a modified decision tree model where every decision node with `samples < threshold` is converted into a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Solution\n",
    "\n",
    "# --- write code here ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (30 points)  \n",
    "Again using sci-kit learn's `DecisionTreeClassifier` class with `random_state=0` and `max_depth=1`, fit a model between features `x_train` and targets `y_train`. Use the function `print_decision_tree(model)` to visually inspect the pruned decision tree model.\n",
    "\n",
    "Using only the printed decision trees, describe an instance of `x` such that the models you trained in part 1  and part 2 would both predict `y=0`, but the model with `max_depth=1` would predict `y=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 Solution\n",
    "\n",
    "# --- write code here ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (20 points)  \n",
    "For every combination of `max_depth` between 1 and 10 and `threshold` between 0 and 350 in increments of 25, train a decision tree classifer on `x_train` and `y_train` and then prune the trained using the `prune_tree` function. Create a scatterplot with `threshold` on the horizontal axis and `max_depth` on the vertical axis, using color to show the classification accuracy of each point. Make 2 scatterplots using this same format, one with training accuracies and the other with validation accuracies.\n",
    "\n",
    "Based on these scatterplots, which values should we select for `max_depth` and `threshold`? If there are multiple models with comparable accuracies, describe other criteria we may want to consider when selecting a decision tree model.\n",
    "\n",
    "Hint: You may find it easier to use `plt.scatter` or `ax.scatter` than `sns.scatterplot` for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4 Solution\n",
    "\n",
    "# --- write code here ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 (20 Points)\n",
    "In this problem you'll explore an alternative hyperparameter search strategy, random search. For each of 100 iterations train a decision tree classifier on `x_train` and `y_train` using a `max_depth` sampled from a discrete uniform distribution between 1 and 9. Then prune the trained model using a `threshold` sampled from a uniform distribution between 0 and 350. Construct 2 plots identical to the plots you produced in part 4 using these new randomly sampled hyperparameters and models.\n",
    "\n",
    "Do the results of this random search change your selection from part 4? If not, describe a setting in which random search would be preferable to grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5 Solution\n",
    "\n",
    "# --- write code here ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
