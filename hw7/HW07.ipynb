{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7 - CS348 Spring 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description** - In this assignment, you will implement and analyze the Expectation Maximization (EM) algorithm for a Gaussian Mixture Model (GMM). You will use then use this model as a joint density estimate.\n",
    "\n",
    "**Getting Started** - You should complete the assignment using your own installation of Python 3 and the packages numpy, pandas, scipy, matplotlib, and seaborn. Download the assignment from Moodle and unzip the file. This will create a directory with this file, 'HW07.ipynb'.\n",
    "\n",
    "**Deliverables** - The assignment has a single deliverable: this jupyter notebook file saved as a pdf. Please answer all coding and writing questions in the body of this file. Once all of the answers are complete, download the file by navigating the following menus: File -> Download as -> PDF via LaTeX. Submit the downloaded pdf file on gradescope. Alternatively, you can save the file as a pdf via the following: File -> Print Preview -> Print as pdf.\n",
    "\n",
    "**Data Sets** - In this assignment, you will find one dataset in the `data` folder, `gmm.csv`.\n",
    "\n",
    "**Academic Honesty Statement** - Copying solutions from external sources (books, web pages, etc.) or other students is considered cheating. Sharing your solutions with other students is considered cheating. Posting your code to public repositories such as GitHub is also considered cheating. Any detected cheating will result in a grade of 0 on the assignment for all students involved, and potentially a grade of F in the course. \n",
    "\n",
    "This academic honesty statement does not restrict you from reading official documentation or using other web resources for understanding the syntax of python, related data science libraries, or properties of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not import any other libraries other than those listed here. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you'll implement and analyze the Expectation Maximization algorithm applied to a gaussian mixture model. The particular gaussian mixture model is described below in the `dgp` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgp(n_instances, n_mixtures, d=2):\n",
    "\n",
    "    # This data generating process generates instances (X) from a \n",
    "    # mixture of gaussian distribuions with randomly generated mean values.\n",
    "    \n",
    "    # Returns \n",
    "    # X               : Numpy array of data instances. size = (n_instances, d)\n",
    "    # mixture_ids     : Numpy array of mixture_id assignment for each data instance. size = (n_instances, )\n",
    "    # mixture_weights : Numpy array of proportion of data generated from each mixture. size = (n_clusters, )\n",
    "    # mixture_means   : Numpy array of the coordinate center of each gaussian mixture. size = (n_mixtures, d)\n",
    "    \n",
    "    X_list = []\n",
    "    mixture_ids_list = []\n",
    "    \n",
    "    # Randomly sample the proportion of Xs drawn from each mixture component.\n",
    "    # The dirichlet distribution produces a vector with elements that sum to 1.\n",
    "    mixture_weights = np.random.dirichlet([4] * n_mixtures)\n",
    "    \n",
    "    # Randomly sample the mean of each mixture component.\n",
    "    mixture_means = np.random.uniform(0, 10, size=(n_mixtures, d))\n",
    "\n",
    "    rounding_error = 0\n",
    "    \n",
    "    for i in range(n_mixtures):\n",
    "        # Determine how many samples to generate for the ith mixture component.\n",
    "        n_instances_mixture = int(n_instances * mixture_weights[i])\n",
    "        \n",
    "        # Hacky method for making sure the total number of instances equals n_instances.\n",
    "        rounding_error += ((n_instances * mixture_weights[i]) - n_instances_mixture)\n",
    "        if rounding_error > 0.5:\n",
    "            n_instances_mixture += 1\n",
    "            rounding_error -= 1\n",
    "        \n",
    "        # Randomly generate Xs from a standard multivariate normal distribution.\n",
    "        X_list.append(np.random.normal(loc=mixture_means[i], scale=1, size=(n_instances_mixture, d)))\n",
    "        \n",
    "        # Record the mixture ids\n",
    "        mixture_ids_list.append(np.array([i] * n_instances_mixture))\n",
    "        \n",
    "    X = np.concatenate(X_list)\n",
    "    mixture_ids = np.concatenate(mixture_ids_list)\n",
    "    \n",
    "    return X, mixture_ids, mixture_weights, mixture_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (10 points)  \n",
    "Implement the function `plot_gmm` below. This function should produce a 5 inch by 5 inch scatter plot with the first column of `X` on the horizontal axis and the second column of `X` on the vertical axis. For each instance in `X`, use color to show its `mixture_id`. Overlaying `X` on this plot, show the mean of each gaussian mixture as a red dot with size proportional to its mixture weight. Label the horizontal axis as `X_1` and label the vertical as `X_2`. \n",
    "\n",
    "After you have implemented the function, use `plot_gmm` to repeatedly visualize 1000 data instances from `dgp` using 4 mixture components. Assuming you didn't know the `mixture_ids`, does the `dgp` always produce data instances that can be visually seperated into 4 distinct groups? If so why, and if not why not? Show a plot from `plot_gmm` to support your argument.\n",
    "\n",
    "Note 1: Make sure that the centers for each gaussian are large enough to be visible but not so large that they cover a large number of data instances. You also may find it helpful to use the `alpha` argument of `plt.scatter` for these red dots.\n",
    "\n",
    "Note 2: Even though the assignment asks you to plot multiple sets of data instances for the `dgp`, your submission only needs to show one of these plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 Solution\n",
    "\n",
    "# --- write code here ---\n",
    "def plot_gmm(X, mixture_ids, mixture_weights, mixture_means):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (15 points)  \n",
    "Implement the function `E_step` below. This function should take as an input a dataset of instances, `X`, the coordinate center of each normal distribution, `mixture_means`, and the weight of each mixure component, `mixture_weights`. It should return `r`, a numpy array of size = (`n_instances`, `n_mixtures`). See slide 43 of the lecture slides on 3/28 for details.\n",
    "\n",
    "Use your implementation of `E_step` with `X`, `mixture_means`, and `mixture_weights` from the same dataset you plotted in part 1. Reproduce the plot you made in part 1, replacing `mixture_ids` with `mixture_ids_pred` which can be computed using `r.argmax(axis=1)`. \n",
    "\n",
    "Does this plot differ from the plot you made in part 1? If so, describe how and why it differs. If not, explain why it appears the same.\n",
    "\n",
    "Note: All of the inputs and outputs of the `E_step` function should be in the same format as the corresponding returned value in `dgp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Solution\n",
    "\n",
    "# --- write code here ---\n",
    "def E_step(X, mixture_means, mixture_weights, n_mixtures):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (15 points)  \n",
    "Implement the function `M_step` below. This function should take as an input a dataset of instances, `X`, their corresponding probabilities of being assigned to each mixture component, `r`, as well as an integer describing the number of mixture components, `n_mixture`. It should return `mixture_means_pred`, the predicted coordinate centers of each of the `n_mixture` mixture components, and `mixture_weights_pred`, the predicted weight of each of the mixture components. See slide 44 of the lecture slides on 3/28 for details.\n",
    "\n",
    "Use your implementation of `M_step` with `X`, `mixture_ids`, and `n_mixtures` from the same dataset you plotted in part 1, using the provided function `convert_mixture_ids_to_r` to produce the input to the `M_step` function from `mixture_ids`. Reproduce the plot you made in part 1, replacing `mixture_means` with `mixture_means_pred` and `mixture_weights` with `mixture_weights_pred`. \n",
    "\n",
    "Does this plot differ from the plot you made in part 1? If so, describe how and why it differs. If not, explain why it appears the same.\n",
    "\n",
    "Note: All of the inputs and outputs of the `M_step` function should be in the same format as the corresponding input or returned value in `dgp`.\n",
    "\n",
    "Hint: The data generating process for this problem does not have as many latent variables as shown on slide 44 of the lecture slides on 3/28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 Solution\n",
    "\n",
    "def convert_mixture_ids_to_r(mixture_ids):\n",
    "    r = np.zeros(shape=(X.shape[0], n_mixtures))\n",
    "    for i in range(mixture_ids.shape[0]):\n",
    "        mixture_id = mixture_ids[i]\n",
    "        r[i, mixture_id] = 1\n",
    "    return r\n",
    "\n",
    "# --- write code here ---\n",
    "def M_step(X, r, n_mixtures, d=2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (20 points)  \n",
    "Using your implementations for `E_step` and `M_step`, implement the Expectation Maximization algorithm in the function `EM` below. This function should take as an input a dataset of instances, `X`, the number of mixture components, `n_mixtures`, and the number of iterations, `n_iterations`. It should return `mixture_ids_pred`, the predicted mixture assignment for each instance in `X`, `mixture_weights_pred`, the predicted proportion of instances generated from each mixture component, and `mixture_means_pred`,  the predicted coordinate centers of each of the mixture components.\n",
    "\n",
    "Use your implementation of `EM` with `X` from the same dataset you plotted in part 1, `n_mixture=4`, and `n_iterations=100`. Reproduce the plot you made in part 1 using the returned arrays from `EM`, replacing `mixture_ids`, `mixture_weights`, and `mixture_means`,  with `mixture_ids_pred`, `mixture_weights_pred`, and `mixture_means_pred` respectively. Repeat this process for multiple executions of `EM`. Is the output of `EM` consistent? If not, explain how and why the output is inconsistent. Be specific.\n",
    "\n",
    "Note: Even though the assignment asks you to plot multiple outputs from `EM`, your submission only needs to show one of these plots.\n",
    "\n",
    "Hint: Your implementation should first generate `mixture_means_pred` randomly, before alternating between executing `E_step` and `M_step` for the specified number of iterations. The key idea with the Expectation Maximization algorithm is that each call to `E_step` and `M_step` uses the current predictions of their arguments, not the ground truth like we used in part 2 and part 3. This style of algorithm is often described as coordinate-ascent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Part 4 Solution\n",
    "\n",
    "# --- write code here ---\n",
    "def EM(X, n_mixtures, n_iterations, d=2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 (20 points)  \n",
    "Using your implementation of `EM`, estimate the number of mixture components, the mixture assignment of each data instance, the mixture weights, and the mixture means using the data instances in `gmm.csv`. Use `plot_gmm` to visually show the final results of this analysis. In your written response, describe how you determined the number of mixture components.\n",
    "\n",
    "Describe a situation in which estimating the number of mixture components in this way is likely to be incorrect.\n",
    "\n",
    "Hint: If you are stuck on the written response for this problem you may find it helpful to review your answers from previous questions in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5 Solution\n",
    "\n",
    "# --- write code here ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 (20 points)  \n",
    "Using your estimates of latent parameters from part 5, estimate the joint density at (4, 4), i.e. P(X_1 = 4, X_2 = 4). \n",
    "\n",
    "How would this density estimate be affected if the situation you described in part 5 had actually occured when generating the data in `dgp.csv`. Explain your answer in 2-4 sentences.\n",
    "\n",
    "Note: The estimated latent parameters include the number of mixture components, the mixture id of each data instance, the weight of each mixture component, and the mean of each mixture component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([4, 4])\n",
    "\n",
    "# Part 6 Solution\n",
    "\n",
    "# --- write code here ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
